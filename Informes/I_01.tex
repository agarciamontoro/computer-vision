\documentclass[a4paper, 11pt]{article}

%Comandos para configurar el idioma
\usepackage[spanish,activeacute]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc} %Necesario para el uso de las comillas latinas.

%Importante que esta sea la última órden del preámbulo
\usepackage{hyperref}
\hypersetup{
  pdftitle={Informe de prácticas - 1},
  pdfauthor={Alejandro García Montoro},
  unicode,
  plainpages=false,
  colorlinks,
  citecolor=black,
  filecolor=black,
  linkcolor=black,
  urlcolor=black,
}

\newcommand\fnurl[2]{%
  \href{#2}{#1}\footnote{\url{#2}}%
}

%Paquetes matemáticos
\usepackage{amsmath,amsfonts,amsthm}
\usepackage{enumerate} %Personalización de enumeraciones
\usepackage{tikz} %Dibujos

%Tipografía escalable
\usepackage{lmodern}
%Legibilidad
\usepackage{microtype}

%Código
\usepackage{listings}
\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=C++,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=left,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{gray},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

\title{Informe de prácticas \\ 1}
\author{Alejandro García Montoro\\
    \href{mailto:agarciamontoro@correo.ugr.es}{agarciamontoro@correo.ugr.es}}
\date{\today}

\theoremstyle{definition}
\newtheorem{ejercicio}{Ejercicio}
\newtheorem*{solucion}{Solución}

\theoremstyle{theorem}
\newtheorem{cuestion}{Cuestión}
\newtheorem{bonus}{Bonus}
\newtheorem{teorema}{Teorema}

\begin{document}

  \maketitle

  \section{Introducción}

  Las prácticas de esta asignatura se implementarán siguiendo el paradigma de orientación a objetos. Por ser esta la primera práctica, se describe aquí la estructura general que se seguirá durante todo el cuatrimestre.

  La implementación gira en torno a la clase Image, que no es más que un \emph{wrapper} de la clase Mat de OpenCV. Sobre ella se definirán los métodos necesarios, siendo públicos sólo los que sean útiles de cara al usuario final.

  Muchos de los métodos privados recibirán y devolverán objetos Mat, que son la base sobre la que se construye Image, mientras que los públicos trabajarán sobre el nivel de abstracción superior que añade esta clase. Además, se podrán definir como funciones \emph{friend} aquellas que no pertenezcan a la clase pero interactúen con ella de una forma profunda.

  Los métodos se declaran en el fichero Image.hpp y se definen en Image.cpp. Reservamos el fichero consts.hpp para declarar y definir tipos de objetos o constantes que se necesiten en el desarrollo de las prácticas.

  El fichero principal es main.cpp, cuyo método main() se actualizará para satisfacer las necesidades de cada práctica. Normalmente, se colocará un waitKey(0) entre los diferentes apartados de la práctica para facilitar su visionado.

  Como primer contacto con la estructura de las prácticas, se expone aquí el estado de la clase Image tras finalizar este primer trabajo:

  \begin{lstlisting}
      class Image{
      private:
          static int num_images;

          Mat image;
          string name;
          int ID;

          void imageInit(string filename, string name, bool flag_color);

          double gaussianFunction(double x, double sigma);
          Mat getGaussMask(double sigma);
          Mat convolution1D(const Mat& signal_vec, const Mat& mask, enum border_id border_type);
          Mat convolution2D(const Mat& signal_mat, const Mat& mask, enum border_id border_type);
          void copyTo(Mat dst);


      public:
          ~Image();

          Image(string filename );
          Image(string filename, string name );
          Image(string filename, bool flag_color);
          Image(string filename, string name, bool flag_color);
          Image(Mat img, string name = "Image");
          Image(const Image& clone);

          const Image operator-(const Image rhs) const;
          const Image operator+(const Image rhs) const;

          int numChannels();
          int rows();
          int cols();
          string getName();
          void setName(string name);

          Image lowPassFilter(double sigma);
          Image highPassFilter(double sigma);
          Image hybrid(Image high_freq, double sigma_low, double sigma_high);

          Image reduceHalf();
          Image pyramidDown(double sigma = 1.0);
          Image makePyramidCanvas(int num_levels);

          Image overlapContours(double low, double high, Scalar color = Scalar(0,0,255));

          void draw();

          friend Image makeHybridCanvas(Image low, Image high, double sigma_low, double sigma_high);
      };
  \end{lstlisting}

  \newpage
  \section{Análisis de la implementación}

  \subsection{Función de convolución}

  La implementación de la función de convolución 2D se construye sobre funciones más simples; a saber:
  \begin{itemize}
      \item Cálculo de una máscara gaussiana 1D.
      \item Convolución 1D con una máscara general.
  \end{itemize}

  Sobre la convolución 2D se construye, además, la función lowPassFilter(), que implementa un filtro Gaussiano.

  \subsubsection*{Cálculo de una máscara gaussiana 1D}
  La máscara gaussiana se construye con la función getGaussMask(), cuyo código se puede ver más abajo. Esta función devuelve un objeto Mat que representa un vector uni-dimensional generado a partir de la discretización de la función gaussiana en el intervalo $[-3\sigma, 3\sigma]$.

  El tamaño de la máscara tiene que ser un número natural impar. Así, primero se redondea el resultado de triplicar el $\sigma$, luego se duplica lo anterior y por último se le añade uno.

  Además, las máscaras de convolución tienen una restricción: la suma de sus valores tiene que ser igual a uno. Por tanto, primero se genera la máscara discretizando la función sin atender a esta restricción y luego se \emph{normaliza}; esto es, se divide cada valor de la máscara por la suma de todos ellos.

  \begin{lstlisting}
      /**
       * Builds a gaussian mask given the parameter sigma. The gaussian function is
       * sampled in the interval [-3*sigma, 3*sigma].
       */
      Mat Image::getGaussMask(double sigma){
          // Gaussian function needs to be sampled between -3*sigma and +3*sigma
          // and the mask has to have an odd dimension.
          int mask_size = 2*round(3*sigma) + 1;

          Mat gauss_mask = Mat(1,mask_size,CV_32FC1);

          // It is necessary to normalize the mask, so the sum of its elements
          // needs to be saved.
          float values_sum = 0;

          // Fills the mask with a sampled gaussian function and saves the sum of all elements
          for (int i = 0; i < mask_size; i++) {
              gauss_mask.at<float>(0,i) = gaussianFunction(i-mask_size/2, sigma);
              values_sum += gauss_mask.at<float>(0,i);
          }

          // Normalizes the gauss mask.
          gauss_mask = gauss_mask / values_sum;

          return gauss_mask;
      }
  \end{lstlisting}

  Los valores de la función gaussiana se consiguen con la siguiente implementación, que da el valor de la función en un punto $x$ con parámetro $\sigma$.
  \begin{lstlisting}
      /**
       * Samples the 1D gaussian function at point x with parameter sigma
       */
      double Image::gaussianFunction(double x, double sigma){
          return exp(-0.5*(x*x)/(sigma*sigma));
      }
  \end{lstlisting}

  \subsubsection*{Convolución 1D con una máscara general}

  El siguiente paso es la implementación de una convolución uni-dimensional con una máscara general. La función devuelve un objeto Mat, que representa el resultado de hacer la convolución del vector uni-dimensional signal\_vec con la máscara mask. El código es el siguiente:

  \begin{lstlisting}
      /**
       * Returns the result of convolving the uni-dimensional signal_vec with the
       * mask, applying one of two types of borders: REFLECT or ZEROS.
       */
      Mat Image::convolution1D(const Mat& signal_vec, const Mat& mask, enum border_id border_type){
          assert(signal_vec.rows == 1 && mask.rows == 1 && mask.cols < signal_vec.cols);

          int num_channels = signal_vec.channels();

          // Initialization of source vector with additional borders.
          int border_size = mask.cols/2;  // Number of pixels added to each side
          Mat bordered;
          copyMakeBorder(signal_vec,bordered,0,0,border_size,border_size,border_type,0.0);

          // Splitting of the bordered vector for making a per-channel processing
          vector<Mat> bordered_channels(num_channels);
          split(bordered, bordered_channels);

          // Declaration of the result vector -with same size and type as the
          // original signal vector- and its splitted channels.
          Mat result = Mat(signal_vec.size(), signal_vec.type());
          vector<Mat> result_channels(num_channels);
          split(result, result_channels);

          // The mask and the source/result channels need to have the same type.
          // They are all converted to CV_32FC1 in order not to lose precision.
          Mat converted_mask;
          mask.convertTo(converted_mask,CV_32FC1);

          // Per-channel processing: we need the source channels, the masked channels;
          // i.e., the source channel focused in a ROI of the same size as the mask
          // and the result channels.
          Mat source_channel, masked_channel, result_channel;

          for (int i = 0; i < num_channels; i++) {
              // Channel type conversion
              bordered_channels[i].convertTo(source_channel, CV_32FC1);
              result_channels[i].convertTo(result_channel, CV_32FC1);

              // Actual processing
              for (int j = 0; j < result.cols; j++) {
                  // We focus on the zone centered at j+mask.cols/2 with mask width
                  masked_channel = source_channel(Rect(j,0,mask.cols,1));

                  // Scalar product between the ROI'd source and the mask
                  result_channel.at<float>(0,j)  = masked_channel.dot(converted_mask);
              }

              // Backwards conversion: the result should have the same type as the input image
              result_channel.convertTo(result_channels[i],result_channels[i].type());
          }

          // Merging again the processed channels
          merge(result_channels, result);

          return result;
      }
  \end{lstlisting}

  El tener que tratar todos los canales de la imagen por separado y después hacer de nuevo la unión hace el código algo más difícil de leer, pero la idea es sencilla:
  \begin{enumerate}
      \item Se genera un vector señal con bordes ---replicados o a ceros, según la decisión del usuario---. El tamaño de estos bordes es igual a la mitad del tamaño de la máscara menos uno.
      \item A cada píxel $j$ del vector resultado ---que tiene tamaño igual al vector señal--- se le asigna el resultado del producto escalar de la máscara con la zona apropiada del vector señal; es decir, con una zona del vector señal de tamaño igual a la máscara y centrada en el píxel $j+mask.cols/2$.
  \end{enumerate}

  Este sencillo algoritmo hay que hacerlo para cada canal, con lo que antes del procesamiento hay que separar el vector señal y el vector resultado y después del procesamiento unir los canales del resultado. Todo este trabajo se hace con las funciones split() y merge() de OpenCV.

  Para la generación del vector señal con bordes se ha usado la función copyMakeBorder(), que hace justo lo que se necesita: generar una matriz con los bordes especificados y del tipo que se deseen. Como su penúltimo argumento recibe una constante de OpenCV especificando qué tipo de borde se desea, en esta implementación que permite sólo dos tipos se ha decidido declarar el siguiente tipo de dato, que es el que recibe la función implementada:

  \begin{lstlisting}
      enum border_id{
          REFLECT = cv::BORDER_REFLECT,
          ZEROS = cv::BORDER_CONSTANT
      };
  \end{lstlisting}

  \subsubsection*{Convolución 2D con una máscara general}

  Esta función simplemente aplica, por filas y columnas, la convolución 1D con la máscara uni-dimensional especificada. Así, devuelve un objeto Mat que representa el resultado de hacer la convolución 2D con la máscara bi-dimensional resultado de hacer el producto matricial de la máscara uni-dimensional consigo misma. El código es el siguiente:

  \begin{lstlisting}
      /**
       * Returns the result of convolving the two-dimensional signal_vec with a
       * uni-dimensional mask, applied in both rows and columns with one of two types
       * of borders: REFLECT or ZEROS.
       */
      Mat Image::convolution2D(const Mat& signal_mat, const Mat& mask, enum border_id border_type){
          Mat result = Mat(signal_mat.size(), signal_mat.type());

          // Row-processing: the uni-dimensional mask is applied to each row separately
          Mat result_row;
          for (int i = 0; i < result.rows; i++) {
              // Row i is replaced with its convolution
              convolution1D(this->image.row(i), mask, border_type).copyTo(result.row(i));
          }

          // Column-processing: the same uni-dimensional mask is applied to each column
          // separately
          Mat transposed_col;
          for (int j = 0; j < result.cols; j++) {
              // Column i is replaced with its convolution ---needs transposing, as convolution1D
              // works with row vectors---.
              transpose(result.col(j),transposed_col);
              transpose(convolution1D(transposed_col, mask, border_type), result.col(j));
          }

          // Returns the convoluted image
          return result;
      }
  \end{lstlisting}

  El código es claro. Lo único que hay que destacar es que para hacer la convolución 1D por columnas lo que se hace es, para cada columna:
  \begin{enumerate}
      \item Trasponer la columna.
      \item Aplicar la convolución 1D con la función anterior.
      \item Trasponer el resultado e insertarlo en el objeto Mat que será devuelto.
  \end{enumerate}

  \subsubsection*{Filtro gaussiano}
  Con las funciones anteriores es entonces directo implementar el filtro gaussiano. Basta calcular la máscara gaussiana dado un $\sigma$ y hacer la convolución 2D de la imagen original y la máscara. El código es el siguiente:

  \begin{lstlisting}
      /*
       * Returns the result of applying a 2D convolution with a gaussian mask that is
       * built with the parameter sigma.
       */
      Image Image::lowPassFilter(double sigma){
          Mat gaussMask = getGaussMask(sigma);

          Mat result  = convolution2D(this->image, gaussMask, REFLECT);

          return Image(result);
      }
  \end{lstlisting}


  \subsection{Imágenes híbridas}
  Una imagen híbrida no es más que el resultado de sumar una imagen a la que se ha aplicado un filtro de paso bajo con otra a la que se le ha aplicado un filtro de paso alto.

  La implementación del filtro de paso bajo es la anterior, así que sólo falta especificar la implementación del filtro de paso alto y de la generación de la imagen híbrida.

  \subsubsection*{Filtro de paso alto}
  El filtro alto implementado es sencillo: se hace la diferencia entre la imagen original y una versión de ella misma a la que se le ha aplicado el filtro de paso bajo. Así, quedan las altas frecuencias de la imagen, que es lo que necesitamos. El código es el siguiente:

  \begin{lstlisting}
      /*
       * Returns the result of subtracting the low-pass-filtered source to the source
       * itself, remaining the high frequencies.
       */
      Image Image::highPassFilter(double sigma){
          return *this - this->lowPassFilter(sigma);
      }
  \end{lstlisting}

  \subsubsection*{Generación de la imagen híbrida}
  La implementación escogida, siguiendo el paradigma de orientación a objetos que se seguirá durante todo el curso, permite llamar al método hybrid() sobre un objeto imagen pasándole como argumento otra imagen. El comportamiento de la función es entonces como sigue: el objeto sobre el que es llamado la función será el usado para generar las frecuencias bajas y, el pasado como argumento, el que se usará para las frecuencias altas.

  La implementación no tiene más misterios, aunque de nuevo su código es menos legible debido a todo el trabajo que hay que hacer por separado con cada canal, además de la división y posterior unión de estos canales. El código es como sigue:
  \begin{lstlisting}
      /*
       * Mixes a low-pass-filtered version of the source with a high-pass-filtered
       * version of high_freq image, returning an hybrid image whose appeareance
       * changes dependening on the distance at which the image is seen.
       */
      Image Image::hybrid(Image high_freq, double sigma_low, double sigma_high){
          assert(this->image.size() == high_freq.image.size());

          Mat result;

          // Applies low-pass filter to the source and high-pass filter to high_freq.
          Image low_passed = this->lowPassFilter(sigma_low);
          Image high_passed = high_freq.highPassFilter(sigma_high);

          // If the number of channels of both images is different, the image with the
          // minimum number of channels (tested with 1) is expanded to an image with the
          // maximum number of channels (tested with 3) copying the first channel.
          if(low_passed.numChannels() != high_passed.numChannels()){
              int max_channels = max(low_passed.numChannels(), high_passed.numChannels());

              //Both images are splitted in a vector with size = maximum number of channels
              vector<Mat> low_channels(max_channels);
              vector<Mat> high_channels(max_channels);

              split(low_passed.image, low_channels);
              split(high_passed.image, high_channels);

              // The image with less channels is expanded
              if (low_passed.numChannels() < max_channels){
                  int diff = max_channels - low_passed.numChannels();

                  for (int i = diff-1; i < max_channels; i++) {
                      low_channels[0].copyTo(low_channels[i]);
                  }
              }
              else if (high_passed.numChannels() < max_channels ) {
                  int diff = max_channels - high_passed.numChannels();

                  for (int i = diff-1; i < max_channels; i++) {
                      high_channels[0].copyTo(high_channels[i]);
                  }
              }

              vector<Mat> result_channels(max_channels);

              // Actual processing
              for (int i = 0; i < max_channels; i++) {
                  result_channels[i] = low_channels[i] + high_channels[i];
              }

              merge(result_channels, result);
          }
          else{
              // Actual processing if the number of channels is the same.
              result = low_passed.image + high_passed.image;
          }

          return Image(result);
      }

  \end{lstlisting}

  Como se ve, el procesamiento real se reduce a una única línea:
  \begin{lstlisting}
      result = low_passed.image + high_passed.image;
  \end{lstlisting}

  La imagen híbrida devuelta es entonces la suma de una imagen con un filtro de paso bajo y otra con un filtro de paso alto.

  \subsubsection*{Dibujo de las tres imágenes}
  Se ha implementado, además, una función adicional que permite generar un \emph{lienzo} con las tres imágenes ---la de frecuencias bajas, la de frecuencias altas y la híbrida--- en una misma imagen.

  Se ha declarado como una función \emph{friend} de la clase, pues no es un método propio del objeto Imagen pero necesita acceder a varios de sus atributos y métodos privados. El código es como sigue:

  \begin{lstlisting}
      /**
       * Returns an image object with the low frequencies image, the high frequencies image and the hybrid image
       * all placed in the same canvas.
       */
      Image makeHybridCanvas(Image low, Image high, double sigma_low, double sigma_high){
          assert(low.image.size() == high.image.size());

          // Generates the low frequencies, high frequencies and hybrid images.
          Image low_passed = low.lowPassFilter(sigma_low);
          Image high_passed = high.highPassFilter(sigma_high);
          Image hybrid = low.hybrid(high,sigma_low,sigma_high);

          // Declare the final canvas
          Mat canvas = Mat(hybrid.rows(),3*hybrid.cols(),hybrid.image.type());

          // Obtain the three ROIs needed to place the images in the canvas
          vector<Mat> slots(3);
          for (int i = 0; i < 3; i++) {
              slots[i] = canvas( Rect(i*hybrid.cols(),0,hybrid.cols(),hybrid.rows()) );
          }

          // Places the images in the canvas ROIs
          low_passed.copyTo(slots[0]);
          high_passed.copyTo(slots[1]);
          hybrid.copyTo(slots[2]);

          return Image(canvas);
      }
  \end{lstlisting}

  La función recibe dos imágenes y los valores de la generación de la función híbrida. Lo único que hace entonces es calcular la imagen de bajas frecuencias, la de altas frecuencias y la híbrida.

  Una vez se han calculado estas imágenes se genera otra cuyo alto es el de la imagen original y cuyo ancho es de tres veces el ancho original. Después de insertar en esta nueva imagen las tres imágenes generadas, se devuelve un objeto Image con este canvas.

  \subsection{Pirámide gaussiana}
  El algoritmo de generación de los niveles de una pirámide gaussiana es sencillo: para cada nivel, se toma el nivel anterior, se le aplica un filtro de paso bajo y se reduce su tamaño a la mitad.

  \subsubsection*{Bajar de nivel en la pirámide gaussiana}

  En vez usar la función pyrDown() de OpenCV, que hace el paso descrito anteriormente, se ha implementado una función equivalente:

  \begin{lstlisting}
      /*
       * Returns the next level in a Gaussian pyramid: it simply blurres the source
       * and then downsamples it by half
       */
      Image Image::pyramidDown(double sigma){
          return this->lowPassFilter(sigma).reduceHalf();
      }
  \end{lstlisting}

  Esta función implementa el algoritmo descrito anteriormente: a la imagen inicial se le aplica un filtro de paso bajo y, después, se reduce su tamaño a la mitad. La implementación del filtro ya la hemos visto, así que sólo queda presentar la función reduceHalf():

  \begin{lstlisting}
      /**
       * Downsamples an image reducing its size by half. The returned image is
       * a copy of the source with odd rows and columns removed.
       */
      Image Image::reduceHalf(){
          Mat dst_rows = Mat(this->rows()/2, this->cols(),this->image.type());
          Mat dst = Mat(this->rows()/2, this->cols()/2,this->image.type());

          // First remove the odd rows
          for (int i = 0; i < dst_rows.rows; i++) {
              this->image.row(2*i).copyTo(dst_rows.row(i));
          }

          // Then, remove the odd columns from the previous output
          for (int i = 0; i < dst.cols; i++) {
              dst_rows.col(2*i).copyTo(dst.col(i));
          }

          return Image(dst);
      }
  \end{lstlisting}

  El código anterior hace una copia de la imagen original y la devuelve con las filas y columnas impares eliminadas. Esto se consigue con dos pasos:
  \begin{enumerate}
      \item Se declara una imagen con la mitad de filas que la original y el mismo número de columnas; se copian en ella las filas impares de la imagen original. \label{uno}
      \item Se declara una image con la mitad de filas \emph{y} la mitad de columnas que la original; se copian en ella las columnas impares de la imagen resultado de \ref{uno}.
  \end{enumerate}

  Esto completa la implementación de la función propia pyramidDown(), que emula el comportamiento de la función pyrDown() de OpenCV.

  \subsubsection*{Dibujo de la pirámide}

  Una vez tenemos esta función construida, la generación de una pirámide con un número arbitrario de niveles es trivial. La única \emph{dificultad} es acoplar todos los niveles en un solo objeto para visualizarlos a la vez.

  El código siguiente implementa esta funcionalidad:

  \begin{lstlisting}
      /*
       * Returns an image filled with a Gaussian pyramid. The number of levels of the
       * pyramid is set by num_levels
       */
      Image Image::makePyramidCanvas(int num_levels){
          // The size of the canvas is:
          //  width:  image.width + image.width/2
          //  height: image.height
          Mat canvas = Mat::zeros(this->rows(),round(this->cols()*1.5),this->image.type());

          // First pyramid level: the source.
          Image pyramid_level = *this;

          // Places the source in the first half of the canvas
          Mat level_zero = canvas( Rect(0,0,this->cols(),this->rows()) );
          pyramid_level.copyTo(level_zero);


          // The next levels of the pyramid are treated in the loop.
          Mat level_i; //ROI where the pyramid level i will be placed

          // ROI variables
          int left, top, width, height;
          left = this->cols(); // All levels (>0) are placed in the second half of the canvas
          top = 0;
          height = 0;

          for (int i = 1; i < num_levels; i++) {
              // Blurred and downsampled image
              pyramid_level = pyramid_level.pyramidDown();

              top    += height; // The top of the ROI is placed just below the previous level
              width  = pyramid_level.cols();
              height = pyramid_level.rows();

              // ROI
              level_i = canvas( Rect(left,top,width,height) );

              // Actual placing
              pyramid_level.copyTo(level_i);
          }

          return Image(canvas);
      }
  \end{lstlisting}

  La idea es sencilla: se crea una imagen con altura igual a la original y anchura igual a 1.5 veces la original. Entonces, el nivel cero de la pirámide, correspondiente a la imagen original sin modificar, se inserta en la parte izquierda del \emph{lienzo}. Por último, se itera sobre el número de niveles especificado y:
  \begin{enumerate}
      \item Se especifica el ROI adecuado sobre el \emph{lienzo}, de manera que todos los niveles tengan su lado izquierdo pegado a la derecha de la imagen original y su parte superior comienze donde termina la parte inferior de la imagen correspondiente al nivel anterior.
      \item Se inserta el nivel actual en el ROI creado.
  \end{enumerate}

  Así, se devuelve un objeto Imagen que contiene todos los niveles de la pirámide gaussiana.

  \subsection{Bonus: detección de bordes}
  Para la consecución del bonus se ha implementado la siguiente función, que devueve un objeto Image con una copia de la imagen original en la que se superponen los contornos encontrados, dibujados con el color especificado por el usuario ---por defecto, rojo---.

  \begin{lstlisting}
      Image Image::overlapContours(double low, double high, Scalar color){
          Mat canny_output;
          vector< vector<Point> > contours;
          vector<Vec4i> hierarchy;

          // Blurs the source to get more accurate contours
          Image source = this->lowPassFilter(1.0);

          // Converts source to gray scale
          Mat gray_source;
          if (this->numChannels() > 1) {
              cvtColor(source.image, gray_source, CV_RGB2GRAY);
          }
          else{
              gray_source = source.image;
          }

          // Detects edges using Canny Filter
          Canny( gray_source, canny_output, low, high );

          // Finds contours and store the result in the contours and hierarchy variables.
          findContours( canny_output, contours, hierarchy, CV_RETR_TREE, CV_CHAIN_APPROX_SIMPLE );

          // Draws red contours
          Image result(*this);

          for( unsigned int i = 0; i< contours.size(); i++ ){
              drawContours( result.image, contours, i, color, 2, 8, hierarchy, 0, Point() );
          }

          return result;
      }
  \end{lstlisting}

  La implementación es trivial con las funciones Canny(), findContours() y drawContours() de OpenCV.

  Lo primero que se hace es aplicar un filtro de paso bajo a la imagen original para mejorar el posterior filtro de Canny. Como este filtro acepta sólo imágenes de un canal, es necesario convertir la imagen original a escala de grises, lo que se consigue con la función propia de OpenCV cvtColor().

  El objeto devuelto por Canny es otra imagen; en particular, otro objeto Mat, pues no es más que el resultado de aplicar una convolución con una máscara específica. Sobre este objeto se ejecuta entonces la función findContours(), que devuelve un vector de contornos ---implementados en OpenCV como un vector de puntos--- y una estructura jerárquica, que especifica la relación entre los contornos anteriores. Así, para cada contorno $i$, las entradas de $hierarchy[i]$ especifican:
  \begin{enumerate}
      \item $hierarchy[i][0]$: siguiente contorno con el mismo nivel jerárquico.
      \item $hierarchy[i][1]$: contorno anterior con el mismo nivel jerárquico.
      \item $hierarchy[i][2]$: primer contorno hijo en la estructura jerárquica.
      \item $hierarchy[i][3]$: contorno padre en la estructura jérarquica.
  \end{enumerate}

  Esta estructura da muchísima información sobre la estructura topológica de la imagen y, aunque en esta práctica no se explota su potencial, es de gran utilidad en aplicaciones de visión por computador donde se quiere sacar significado de los contornos.

  Por último, para cada contorno devuelto por la función findContours(), se ejecuta la función drawContours() sobre la imagen resultado con el color especificado por el usuario. Esto finaliza la implementación pedida.

  \newpage
  \section{Análisis de resultados}

  \subsection{Función de convolución}
  El filtro de paso bajo funciona de la forma esperada, como se aprecia en las figuras \ref{gauss_original} y \ref{gauss_resultado}.

  \begin{figure}[ht!]
      \centering
      \includegraphics[width=40mm]{../imagenes/capturas/I01-00.png}
      \caption{Imagen original. \label{gauss_original}}
  \end{figure}

  \begin{figure}[ht!]
      \centering
      \includegraphics[width=40mm]{../imagenes/capturas/I01-01.png}
      \caption{Imagen tras aplicarle filtro de paso bajo. \label{gauss_resultado}}
  \end{figure}

  \subsection{Imágenes híbridas}
  La generación de imágenes híbridas, por otro lado, depende mucho de las imágenes originales y de los parámetros escogidos. La figura \ref{hibrida_1} es un primer ejemplo de su funcionalidad, donde se ven las imágenes intermedias y la imagen híbrida final.

  \begin{figure}[ht!]
      \centering
      \includegraphics[width=140mm]{../imagenes/capturas/I01-02.png}
      \caption{Imagen híbrida. \label{hibrida_1}}
  \end{figure}

  En este caso, como en el artículo de Oliva, Torralba y Schyns, se ha decidido tomar la imagen de frecuencias bajas en blanco y negro, para que al observar de cerca la imagen híbrida, sus formas se asocien con sombras. Los parámetros usados hacen variar mucho el resultado, y un estudio de su elección queda pendiente.

  Otro ejemplo quizás mejor conseguido es el siguiente, donde las frecuencias altas corresponden a la imagen de un pez y las frecuencias bajas a la de un submarino. En la siguiente sección se ve la pirámide gaussiana construida a partir de este ejemplo, y es claro cómo al disimnuir el tamaño se identifica más el submarino y menos el pez.

  \begin{figure}[ht!]
      \centering
      \includegraphics[width=110mm]{../imagenes/capturas/I01-05.png}
      \caption{Imagen híbrida. \label{hibrida_2}}
  \end{figure}


  \subsection{Pirámide gaussiana}

  La solución de la pirámide gaussiana es tal y como se espera. Tanto la reducción de tamaño como la construcción de la imagen con toda la pirámide son triviales. Mostramos un ejemplo en la figura \ref{piramide}, donde se aprecia, además de la construcción de la pirámide, la bondad del resultado de la imagen híbrida vista anteriormente.

  \begin{figure}[ht!]
      \centering
      \includegraphics[width=130mm]{../imagenes/capturas/I01-03.png}
      \caption{Pirámide gaussiana con imagen híbrida. \label{piramide}}
  \end{figure}

  \subsection{Bonus: detección de bordes}
  La implementación de la detección de los bordes es también muy dependiente de los parámetros que se escojan y de la imagen sobre la que se trabaje. La figura \ref{bordes} refleja esta función en acción, donde se detectan los bordes de un avión de forma lejana a la óptima. Un estudio particular de las imágenes y de los parámetros necesarios para mejorar la detección de los bordes queda pendiente.

  \begin{figure}[ht!]
      \centering
      \includegraphics[width=80mm]{../imagenes/capturas/I01-04.png}
      \caption{Detección de bordes. \label{bordes}}
  \end{figure}

\end{document}
